name: Postgres Backup

on:
  push:
    branches: [ main ]
  workflow_dispatch:
  schedule:
    - cron: '*/2 * * * *'

jobs:
  backup:
    runs-on: ubuntu-latest

    services:
      db:
        image: postgres:17-alpine
        env:
          POSTGRES_USER: bootcamp_admin
          POSTGRES_PASSWORD: secure_password
          POSTGRES_DB: bootcamp_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U bootcamp_admin -d bootcamp_db"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          --health-start-period 60s

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Prepare workspace
        run: |
          set -euo pipefail
          mkdir -p "$GITHUB_WORKSPACE/postgres_backups"
          mkdir -p "$GITHUB_WORKSPACE/postgres_backups/logs"

      - name: Find Postgres service container
        id: find_container
        run: |
          set -euo pipefail
          CONTAINER_ID=$(docker ps --filter "ancestor=postgres:17-alpine" --format "{{.ID}}" | head -n1 || true)
          if [ -z "$CONTAINER_ID" ]; then
            echo "ERROR: Postgres service container not found"
            docker ps --format "table {{.ID}}\t{{.Image}}\t{{.Names}}"
            exit 1
          fi
          echo "container_id=$CONTAINER_ID" >> "$GITHUB_OUTPUT"

      - name: Wait for Postgres ready inside container
        run: |
          set -euo pipefail
          CONTAINER_ID="${{ steps.find_container.outputs.container_id }}"
          echo "Using container $CONTAINER_ID"
          until docker exec "$CONTAINER_ID" pg_isready -U bootcamp_admin -d bootcamp_db > /dev/null 2>&1; do
            echo "Waiting for Postgres inside container..."
            sleep 2
          done
          echo "Postgres is ready"

      - name: Diagnostics (list DBs, table counts) and run verbose pg_dump
        env:
          DB_USER: bootcamp_admin
          DB_PASSWORD: secure_password
          DB_NAME: bootcamp_db
        run: |
          set -euo pipefail
          CONTAINER_ID="${{ steps.find_container.outputs.container_id }}"
          OUT_DIR="$GITHUB_WORKSPACE/postgres_backups"
          LOG_DIR="$OUT_DIR/logs"
          TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)
          DUMPFILE="$OUT_DIR/${DB_NAME}_${TIMESTAMP}.sql.gz"
          DUMPLOG="$LOG_DIR/pg_dump_${TIMESTAMP}.log"
          DIAGLOG="$LOG_DIR/diag_${TIMESTAMP}.log"

          echo "=== Diagnostics: databases and sizes ===" > "$DIAGLOG"
          # List databases
          docker exec -e PGPASSWORD="$DB_PASSWORD" "$CONTAINER_ID" psql -U "$DB_USER" -d postgres -c "\l+" >> "$DIAGLOG" 2>&1 || true

          echo "=== Table count in target DB ===" >> "$DIAGLOG"
          # Count tables in public schema (change if you use other schemas)
          docker exec -e PGPASSWORD="$DB_PASSWORD" "$CONTAINER_ID" psql -U "$DB_USER" -d "$DB_NAME" -t -c "SELECT count(*) FROM information_schema.tables WHERE table_catalog = current_database() AND table_schema NOT IN ('pg_catalog','information_schema');" >> "$DIAGLOG" 2>&1 || true

          echo "Diagnostics written to $DIAGLOG"
          echo "Running pg_dump (verbose) inside container; verbose output will go to $DUMPLOG"

          # Run pg_dump inside the container and capture verbose output
          # Use a pipeline but capture pg_dump exit status via PIPESTATUS-like behavior
          docker exec -e PGPASSWORD="$DB_PASSWORD" "$CONTAINER_ID" bash -lc "pg_dump -v -U '$DB_USER' '$DB_NAME'" > "$OUT_DIR/temp_dump.sql" 2> "$DUMPLOG" || true
          PGDUMP_EXIT=$?
          if [ -s "$OUT_DIR/temp_dump.sql" ]; then
            gzip -c "$OUT_DIR/temp_dump.sql" > "$DUMPFILE"
            rm -f "$OUT_DIR/temp_dump.sql"
          else
            echo "Warning: pg_dump produced no SQL output" >> "$DUMPLOG"
            rm -f "$OUT_DIR/temp_dump.sql" || true
          fi

          if [ "$PGDUMP_EXIT" -ne 0 ]; then
            echo "pg_dump exit code: $PGDUMP_EXIT" >> "$DUMPLOG"
            echo "ERROR: pg_dump failed (see $DUMPLOG)"
            # keep logs and proceed to uploading artifact for inspection, but fail the step
            cat "$DUMPLOG" || true
            exit $PGDUMP_EXIT
          fi

          echo "Backup saved to $DUMPFILE"
          echo "dump_file=$DUMPFILE" >> "$GITHUB_OUTPUT"
          echo "dump_log=$DUMPLOG" >> "$GITHUB_OUTPUT"
          echo "diag_log=$DIAGLOG" >> "$GITHUB_OUTPUT"

      - name: Collect db size and table count for CSV
        id: collect_info
        env:
          DB_USER: bootcamp_admin
          DB_PASSWORD: secure_password
          DB_NAME: bootcamp_db
        run: |
          set -euo pipefail
          CONTAINER_ID="${{ steps.find_container.outputs.container_id }}"
          # db size in bytes
          db_size=$(docker exec -e PGPASSWORD="$DB_PASSWORD" "$CONTAINER_ID" psql -U "$DB_USER" -d "$DB_NAME" -t -c "SELECT COALESCE(pg_database_size(current_database()),0);" | tr -d '[:space:]' || echo "")
          # table count
          table_count=$(docker exec -e PGPASSWORD="$DB_PASSWORD" "$CONTAINER_ID" psql -U "$DB_USER" -d "$DB_NAME" -t -c "SELECT count(*) FROM information_schema.tables WHERE table_catalog = current_database() AND table_schema NOT IN ('pg_catalog','information_schema');" | tr -d '[:space:]' || echo "")
          echo "db_size_bytes=${db_size:-}" >> "$GITHUB_OUTPUT"
          echo "table_count=${table_count:-}" >> "$GITHUB_OUTPUT"
          echo "Collected db_size_bytes=${db_size:-} table_count=${table_count:-}"

      - name: Generate backups CSV (include db size and table count)
        run: |
          set -euo pipefail
          OUT_DIR="$GITHUB_WORKSPACE/postgres_backups"
          OUT="$OUT_DIR/backups.csv"
          dumpfile="${{ steps.find_container.outputs.dump_file || '' }}"
          # If dump path wasn't set via output, find newest gz
          if [ -z "${{ steps.find_container.outputs.dump_file || '' }}" ]; then
            if compgen -G "$OUT_DIR/*.gz" > /dev/null; then
              dumpfile=$(ls -1t "$OUT_DIR"/*.gz | head -n1)
            else
              dumpfile=""
            fi
          fi

          filename=$(basename "$dumpfile" 2>/dev/null || echo "")
          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          size_bytes=0
          if [ -n "$dumpfile" ] && [ -f "$dumpfile" ]; then
            size_bytes=$(stat -c %s "$dumpfile" || echo 0)
          fi

          db_size="${{ steps.collect_info.outputs.db_size_bytes || '' }}"
          table_count="${{ steps.collect_info.outputs.table_count || '' }}"
          dump_log="${{ steps.find_container.outputs.dump_log || '' }}"
          diag_log="${{ steps.find_container.outputs.diag_log || '' }}"

          echo "filename,path,timestamp,size_bytes,db_size_bytes,table_count,dump_log,diag_log" > "$OUT"
          echo "${filename},postgres_backups,${timestamp},${size_bytes:-0},${db_size:-},${table_count:-},${dump_log:-},${diag_log:-}" >> "$OUT"

          echo "Generated CSV: $OUT"
          ls -la "$OUT_DIR" || true
          sed -n '1,200p' "$OUT" || true

      - name: Upload backup artifact
        uses: actions/upload-artifact@v4
        with:
          name: postgres-backups
          path: ${{ github.workspace }}/postgres_backups/**